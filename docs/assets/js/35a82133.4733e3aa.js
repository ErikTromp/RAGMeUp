"use strict";(self.webpackChunkrag_me_up_documentation=self.webpackChunkrag_me_up_documentation||[]).push([[373],{6884:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>h,contentTitle:()=>l,default:()=>a,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"env","title":"Configuring the .env File","description":"RAG Me Up implements all the components described in the High-Level Design and lets you configure them, possibly even turning some components completely off.","source":"@site/docs/env.md","sourceDirName":".","slug":"/env","permalink":"/env","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/env.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"High-Level Design","permalink":"/architecture"},"next":{"title":"Running RAG Me Up","permalink":"/running"}}');var s=t(4848),r=t(8453);const o={sidebar_position:4},l="Configuring the .env File",h={},d=[{value:"Logging",id:"logging",level:2},{value:"Document embedding",id:"document-embedding",level:2},{value:"Data loading",id:"data-loading",level:2},{value:"Chunking",id:"chunking",level:2},{value:"Database setup",id:"database-setup",level:2},{value:"Reranking",id:"reranking",level:2},{value:"HyDE",id:"hyde",level:2},{value:"Conversation summarization",id:"conversation-summarization",level:2},{value:"RAG configuration",id:"rag-configuration",level:2},{value:"Rewrite loop",id:"rewrite-loop",level:2},{value:"Re2",id:"re2",level:2},{value:"Provenance",id:"provenance",level:2},{value:"Model selection",id:"model-selection",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"configuring-the-env-file",children:"Configuring the .env File"})}),"\n",(0,s.jsxs)(n.p,{children:["RAG Me Up implements all the components described in the ",(0,s.jsx)(n.a,{href:"/architecture",children:"High-Level Design"})," and lets you configure them, possibly even turning some components completely off.\r\nThis is all done through the ",(0,s.jsx)(n.code,{children:".env"})," file (in the ",(0,s.jsx)(n.code,{children:"/server"})," folder), for which you can find a template version in the repository named ",(0,s.jsx)(n.code,{children:".env.template"}),". Just rename this file to ",(0,s.jsx)(n.code,{children:".env"})," and configure your\r\nRAG Me Up by following this page's documentation."]}),"\n",(0,s.jsxs)(n.p,{children:["The following components can be configured through the ",(0,s.jsx)(n.code,{children:".env"})," file."]}),"\n",(0,s.jsx)(n.h2,{id:"logging",children:"Logging"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"logging_level"})," can be set through this variable. Use ",(0,s.jsx)(n.code,{children:"DEBUG"})," for local development and testing and something like ",(0,s.jsx)(n.code,{children:"WARN"})," or ",(0,s.jsx)(n.code,{children:"ERROR"})," for production settings."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"document-embedding",children:"Document embedding"}),"\n",(0,s.jsxs)(n.p,{children:["Embeddings are done ",(0,s.jsx)(n.strong,{children:"locally"})," in RAG Me Up, meaning we don't use OpenAI, Anthropic or other API providers for embeddings but instead directly use Huggingface models to do the embedding.\r\nThey can either be run on GPU (fast, if you have one) or on CPU (slow, but always possible)."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"embedding_model"})," sets the Huggingface model to use. Use leaderboards like ",(0,s.jsx)(n.a,{href:"https://huggingface.co/spaces/mteb/leaderboard",children:"MTEB"})," to decide what model you want to use. Try to find a trade-off between size/embedding dimension (hence, speed) and accuracy."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"embedding_cpu"})," can be used to force CPU usage, by default RAG Me Up will use a GPU."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"data-loading",children:"Data loading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"data_directory"})," should be the full or relative path to where your data is stored. RAG Me Up will use this when it is run for the first time to read in all the supported files once and load them into the database. Once there is data present in the database, it will not load the data from this folder yet again."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"file_types"})," is a comma-separated list of file types (extensions) that should be loaded. RAG Me Up currently supports ",(0,s.jsx)(n.code,{children:"pdf,json,docx,pptx,xslx,csv,txt"}),". If you leave one of these out, files with that extension, while maybe present in your data directory, will not be loaded."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"json_schema"})," if JSON files are to be processed, you can specify a custom schema to load specific parts using ",(0,s.jsx)(n.a,{href:"https://jqlang.org/",children:"jq"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"csv_seperator"})," if CSV files are to be processed, this should be the separator."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"chunking",children:"Chunking"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"splitter"})," should be the name of the splitter to use. Allowed are: ",(0,s.jsx)(n.code,{children:"RecursiveCharacterTextSplitter"}),", ",(0,s.jsx)(n.code,{children:"SemanticChunker"})," or ",(0,s.jsx)(n.code,{children:"ParagraphChunker"})," (a RecursiveCharacterTextSplitter that always respects paragraph boundaries)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"recursive_splitter_chunk_size"})," sets the chunk size for the RecursiveCharacterTextSplitter."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"recursive_splitter_chunk_overlap"})," sets the overlap size for the RecursiveCharacterTextSplitter."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"semantic_chunker_breakpoint_threshold_type"})," sets the threshold type for the SemanticChunker. Allowed are: ",(0,s.jsx)(n.code,{children:"percentile"}),", ",(0,s.jsx)(n.code,{children:"standard_deviation"}),", ",(0,s.jsx)(n.code,{children:"interquartile"}),", ",(0,s.jsx)(n.code,{children:"gradient"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"semantic_chunker_breakpoint_threshold_amount"})," set the treshold amount for the SemanticChunker."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"semantic_chunker_number_of_chunks"})," set the number of chunks for the SemanticChunker."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"paragraph_chunker_max_chunk_size"})," set the chunk size for the ParagraphChunker."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"paragraph_chunker_paragraph_separator"})," set the separator size for the ParagraphChunker. This will be used to determine what a paragraph is by splitting on it as regex."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"database-setup",children:"Database setup"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"postgres_uri"})," should specify the URI for the Postgres instance to use. Check the ",(0,s.jsx)(n.code,{children:"postgres"})," subfolder for a Docker that will run an image that can function as a hybrid retrieval store."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"vector_store_k"})," is the number of document chunks to fetch (during normal retrieval) from the Postgres database."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"reranking",children:"Reranking"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rerank"})," is a boolean that you can set to turn reranking on or off."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rerank_k"})," the number of documents to keep after reranking. Usually you set ",(0,s.jsx)(n.code,{children:"vector_store_k"})," relatively high and ",(0,s.jsx)(n.code,{children:"rerank_k"})," to be your final desired amount of chunks to keep."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rerank_model"})," the (flashrank) rerank model to use, find alternatives on Huggingface but be sure they are flashrank-compatible."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"hyde",children:"HyDE"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"use_hyde"})," boolean to turn HyDE on or off."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"hyde_query"})," the prompt given to the LLM to let it generate a hypothetical document. Must have the following placeholders:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"question"})," the user question."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"conversation-summarization",children:"Conversation summarization"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"use_summarization"})," is a boolean turning automatic summarization (of the conversation history) on or off."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"summarization_threshold"})," is the number of tokens (",(0,s.jsx)(n.strong,{children:"not"})," characters) the history should exceed to start summarization. Be sure to keep a buffer between this threshold and your model's context window because there will be some overhead from the summarization prompt."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"summarization_query"})," this is the query that will be sent to the LLM to run the actual summarization. Must have the following placeholders:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"history"})," containing the actual conversation history, best put at the end of the prompt."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"summarization_encoder"})," the ",(0,s.jsx)(n.a,{href:"https://github.com/openai/tiktoken",children:"tiktoken"})," model to use to count the tokens for the ",(0,s.jsx)(n.code,{children:"summarization_threshold"})," check."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"rag-configuration",children:"RAG configuration"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"temperature"})," should be the model temperature. 0 for no variation and higher for more variation."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rag_instruction"})," the system prompt/instruction to use for normal RAG query answering. It is wise to include some background on the RAG system's purpose and always try to force the system to mention sources used. Must have the following placeholders:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"context"})," the documents retrieved from the Postgres database."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rag_question_initial"})," decoration around the user's question that will be asked to the LLM. RAG Me Up allows to differentiate initial and follow-up questions through different prompts. Must have the following placeholders:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"question"})," the original user question."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rag_question_followup"})," same as above but for a followup question. Must have the following placeholders:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"question"})," the user's follow-up question."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rag_fetch_new_question"})," the prompt sent to the LLM to ask to check for whether or not we should fetch new documents, given that we have a follow-up already. This prompt ",(0,s.jsx)(n.strong,{children:"must"})," force the LLM to answer with yes (should fetch) or no (no fetch required) ",(0,s.jsx)(n.strong,{children:"only"}),". Must have the following placeholders:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"question"})," the user's follow-up question."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"rewrite-loop",children:"Rewrite loop"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"use_rewrite_loop"})," a boolean indicating whether or not we should use the rewrite-loop (only once) to check if the retrieved documents can be used to answer the user's question."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rewrite_query_instruction"})," the system prompt sent to the LLM to decide if the documents can answer the question. Must contain the retrieved documents. This prompt ",(0,s.jsx)(n.strong,{children:"must"})," force the LLM to answer with yes (documents can answer the question) or no (we should rewrite the question), followed by a motivation. This motivation will be used in the actual rewriting. Must have the following placeholders:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"context"})," the documents retrieved from the Postgres database."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rewrite_query_question"})," the message sent to the LLM containing the user's question that should be answered with the documents. Must have the following placeholders:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"question"})," the user question."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"rewrite_query_prompt"})," this prompt is used to instruct the LLM to perform the actual rewrite. It is adviced to let the LLM only answer with the rephrasing and not let it add decorations or explanations. Must have the following placeholders:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"question"})," the user question to rewrite."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"motivation"})," the motivation output from the earlier query asking the LLM whether the documents can be used to answer the question or not."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"re2",children:"Re2"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"use_re2"})," boolean indicating whether or not to use re-reading instruction."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"re2_prompt"})," the prompt that will be injected in between the re-iterating of the question. This will result in the following format: ",(0,s.jsx)(n.code,{children:"[Original question]\\n{re2_prompt}\\n[Original question]"})]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"provenance",children:"Provenance"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"provenance_method"})," the provenance attribution metric to use. Allowed values are: ",(0,s.jsx)(n.code,{children:"rerank"}),", ",(0,s.jsx)(n.code,{children:"similarity"}),", ",(0,s.jsx)(n.code,{children:"llm"})," or ",(0,s.jsx)(n.code,{children:"None"})," (to turn provenance attribution off)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"provenance_similarity_llm"})," is the model to use when applying ",(0,s.jsx)(n.code,{children:"similarity"})," provenance attribution to compute the similarities of the documents to the answer (and question)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"provenance_include_query"})," by default provenance is attributed to the answer only. Set this flag to True to ",(0,s.jsx)(n.strong,{children:"also"})," attribute to the question."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"provenance_llm_prompt"})," is the prompt used to ask the LLM for provenance when the provenance_method is set to LLM. You are free to define any ranking score or mechanism but do make this really clear in the prompt. Must have the following placeholders:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"query"})," the user question."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"answer"})," the answer to the question as generated by the LLM."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"context"})," the document chunk that we are attributing provenance for."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"model-selection",children:"Model selection"}),"\n",(0,s.jsxs)(n.p,{children:["You can choose between different LLM providers, including running your own locally through ",(0,s.jsx)(n.a,{href:"https://ollama.com/",children:"Ollama"}),". Make sure that you set all environment variables that are required for your specific provider (eg. the ",(0,s.jsx)(n.code,{children:"OPENAI_API_KEY"})," for OpenAI)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"use_openai"})," set to True to use OpenAI."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"openai_model_name"})," the model to use when selecting OpenAI."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"use_azure"})," set to True to use Azure OpenAI."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"use_gemini"})," set to True to use Gemini."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"gemini_model_name"})," the Gemini model to use when selecting Gemini."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"use_anthropic"})," set to True to use Anthropic."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"anthropic_model_name"})," the Antrhopic model to use when selecting Anthropic."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"use_ollama"})," set to True to use Ollama (local model)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"ollama_model_name"})," the Ollama model to use when selecting Ollama. Look for models ",(0,s.jsx)(n.a,{href:"https://ollama.com/search",children:"here"})]}),"\n"]})]})}function a(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>l});var i=t(6540);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);