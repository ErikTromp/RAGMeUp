"use strict";(self.webpackChunkrag_me_up_documentation=self.webpackChunkrag_me_up_documentation||[]).push([[443],{5874:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>h,contentTitle:()=>a,default:()=>l,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"architecture","title":"High-Level Design","description":"The full high-level overview of RAG Me Up can be found in the drawing below. This image shows all components that can be used in the framework but all of them","source":"@site/docs/architecture.md","sourceDirName":".","slug":"/architecture","permalink":"/architecture","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/architecture.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"An introduction to RAG (and AI)","permalink":"/intro_rag"},"next":{"title":"Configuring the .env File","permalink":"/env"}}');var i=n(4848),r=n(8453);const o={sidebar_position:3},a="High-Level Design",h={},c=[{value:"High Level Pipeline Explanation",id:"high-level-pipeline-explanation",level:2},{value:"Your Documents",id:"your-documents",level:3},{value:"Loaders",id:"loaders",level:3},{value:"Chunking",id:"chunking",level:3},{value:"Document Embeddings",id:"document-embeddings",level:3},{value:"Sparse BM25 Vector store",id:"sparse-bm25-vector-store",level:3},{value:"Dense Vector DB",id:"dense-vector-db",level:3},{value:"Query (User Interface)",id:"query-user-interface",level:3},{value:"History Summarization",id:"history-summarization",level:3},{value:"Document Fetch Check",id:"document-fetch-check",level:3},{value:"HyDE",id:"hyde",level:3},{value:"Document Retrieval",id:"document-retrieval",level:3},{value:"Reranking",id:"reranking",level:3},{value:"Answer Check - LLM-as-a-judge",id:"answer-check---llm-as-a-judge",level:3},{value:"Re2",id:"re2",level:3},{value:"Prompt Creation",id:"prompt-creation",level:3},{value:"Provenance Attribution",id:"provenance-attribution",level:3}];function d(e){const t={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",section:"section",strong:"strong",sup:"sup",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"high-level-design",children:"High-Level Design"})}),"\n",(0,i.jsxs)(t.p,{children:["The full high-level overview of RAG Me Up can be found in the drawing below. This image shows all components that ",(0,i.jsx)(t.em,{children:"can"})," be used in the framework but all of them\r\nare configurable or can be turned on/off depending on your needs. Deciding which components you want to use and how to configure them, is crucial in setting up\r\nproduction-grade RAG pipelines and will be explained for each component separately."]}),"\n",(0,i.jsxs)(t.p,{children:["The entire RAG pipeline that will be executed by RAG Me Up can be configured through the ",(0,i.jsx)(t.code,{children:".env"})," file entirely. An example is given in ",(0,i.jsx)(t.code,{children:".env.template"})," which you\r\ncan rename to ",(0,i.jsx)(t.code,{children:".env"}),". The template aims to provide a sane starting point for generic RAG but should always be subject to tweaking when building your RAG setup."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"RAG pipeline drawing",src:n(8979).A+""})}),"\n",(0,i.jsx)(t.h2,{id:"high-level-pipeline-explanation",children:"High Level Pipeline Explanation"}),"\n",(0,i.jsx)(t.p,{children:"Any RAG framework or pipeline roughly consists of two distinct sections:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["Indexing - a one-time process where a large corpus of documents is processed and indexed into a (vector) database. The whole indexing phase is executed ",(0,i.jsx)(t.em,{children:"before"})," the query phase takes place."]}),"\n",(0,i.jsx)(t.li,{children:"Querying - a runtime process where a user initiaties a retrieval and answer process through an interaction. In this process, the indexes created during the indexing phase are used to retrieve documents and use those to answer the user's query."}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"While all of the components shown in the diagram will be discussed in the remainder in detail, we briefly address each below with links to subsections you can explore if you don't want to read the documentation in full. Mind you that when using RAG Me Up, you will first and foremost be configuring the .env file which is a single place to set up all the components shown here."}),"\n",(0,i.jsx)(t.h3,{id:"your-documents",children:"Your Documents"}),"\n",(0,i.jsxs)(t.p,{children:["Of course any RAG pipeline starts with your documents. While there are different flavors of RAG (GraphRAG, Text2SQL, etc.), RAG Me Up focuses on semantic RAG. This means that your documents and the queries you want to run on them should be semantic ones. Hence, your documents should contain some form of ",(0,i.jsx)(t.em,{children:"text"}),". Whether that is inside a PDF, DOCX, JSON, CSV, etc. doesn't matter but RAG Me Up does focus on text."]}),"\n",(0,i.jsx)(t.h3,{id:"loaders",children:"Loaders"}),"\n",(0,i.jsx)(t.p,{children:"For each document type (DOCX vs XLSX vs JSON, etc.), the way to retrieve the text from the source document differs. Hence, we have loaders for each type of document separately."}),"\n",(0,i.jsx)(t.h3,{id:"chunking",children:"Chunking"}),"\n",(0,i.jsxs)(t.p,{children:['A strength of RAG (when compared to "just" using an LLM and uploading a document) is that it is capable to search through large amounts of documents and also search ',(0,i.jsx)(t.strong,{children:"within"})," those documents. To achieve the latter, documents are chopped up into chunks. There are different ways of doing this chopping up and RAG Me Up supports a few."]}),"\n",(0,i.jsx)(t.h3,{id:"document-embeddings",children:"Document Embeddings"}),"\n",(0,i.jsxs)(t.p,{children:["Once we have our data chunked into parts of text, arguably the most crucial step is to convert them to vectors that we can use for comparison during query-time. RAG Me Up by uses hybrid search which combines dense and sparse vectors. The document embeddings are used to create the ",(0,i.jsx)(t.strong,{children:"dense"})," vectors using an LM or LLM",(0,i.jsx)(t.sup,{children:(0,i.jsx)(t.a,{href:"#user-content-fn-1",id:"user-content-fnref-1","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"1"})}),"."]}),"\n",(0,i.jsx)(t.h3,{id:"sparse-bm25-vector-store",children:"Sparse BM25 Vector store"}),"\n",(0,i.jsxs)(t.p,{children:["One of the two ways chunks are indexed in RAG Me Up is using ",(0,i.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Okapi_BM25",children:"BM25"})," as sparse vectors which typically excell at keyword-type search. This is useful for example when users query our RAG system with very brief, perhaps even single-word, queries that do not necessarily lead to meaningful embeddings."]}),"\n",(0,i.jsxs)(t.p,{children:["RAG Me Up uses Postgres for storing sparse vectors with a BM25 index using ",(0,i.jsx)(t.a,{href:"https://docs.paradedb.com/welcome/introduction",children:"pg_search by ParadeDB"}),"."]}),"\n",(0,i.jsx)(t.h3,{id:"dense-vector-db",children:"Dense Vector DB"}),"\n",(0,i.jsxs)(t.p,{children:["The other way chunks are indexed in RAG Me Up is by writing the document embeddings created in the step before into an indexed database. For this, RAG Me Up also uses Postgres, indexed with the ",(0,i.jsx)(t.a,{href:"https://github.com/pgvector/pgvector",children:"pgvector extension"}),"."]}),"\n",(0,i.jsx)(t.h3,{id:"query-user-interface",children:"Query (User Interface)"}),"\n",(0,i.jsx)(t.p,{children:"Not explicitly written out in the diagram is the user interface. RAG Me Up comes with a custom user interface written in Scala to allow users to talk to the RAG system. RAG Me Up's server (Python) is built to be stateless which means that the chat history, memory, previously retrieved documents, etc. are all supposed to be handled by the user interface."}),"\n",(0,i.jsx)(t.p,{children:"An important part of the query handling step is to convert the user's question into vectors that are similar to what is stored in the (vector) database. While not explicitly drawn in the architecture, we use the same embedding model to create the dense vector for the query."}),"\n",(0,i.jsx)(t.h3,{id:"history-summarization",children:"History Summarization"}),"\n",(0,i.jsx)(t.p,{children:"While context windows of LLMs are ever increasing, they are still limited and even if they are really large, it becomes increasingly harder for an LLM to focus on the essence of a message as the size increases. Mind you also that in case of RAG Me Up, the chat history is part of every message sent to the LLM."}),"\n",(0,i.jsx)(t.p,{children:"To remedy this, you can optionally summarize lengthy messages (with history) once they exceed a specific threshold."}),"\n",(0,i.jsx)(t.h3,{id:"document-fetch-check",children:"Document Fetch Check"}),"\n",(0,i.jsx)(t.p,{children:"When dealing with history in a RAG system, it is important to determine whether or not a question from a user is actually a follow-up on documents that were already retrieved or not. New documents should only be fetched if there is either no history present yet or when the user's question calls for a new retrieval. We make this decision by asking an LLM which way we should go in case there is already history present."}),"\n",(0,i.jsx)(t.p,{children:"If documents should be fetched, we continue to retrieval. If not, we go to answering the question directly with documents that were previously retrieved and are present in the chat history."}),"\n",(0,i.jsx)(t.h3,{id:"hyde",children:"HyDE"}),"\n",(0,i.jsxs)(t.p,{children:["When using any RAG system, there is an inherent mismatch between the indexing phase and the query phase. When indexing our document chunks, we are essentially working with (potential) ",(0,i.jsx)(t.em,{children:"answers"}),". Whenever a user poses a query, this is a ",(0,i.jsx)(t.em,{children:"question"}),". When we just naively embed the chunks and the question in exactly the same way, we are comparing apples to oranges, though we expect there to be some coherence between the two."]}),"\n",(0,i.jsx)(t.p,{children:"Nonetheless, HyDE (Hypothetical Document Embeddings) tries to remedy this by generating a couple of documents with a given query. We do this by asking the LLM to generate those documents. This way we hope to compare apples with apples by using the answers generated with the query instead for retrieving relevant document chunks. HyDE is an optional step in RAG Me Up."}),"\n",(0,i.jsx)(t.h3,{id:"document-retrieval",children:"Document Retrieval"}),"\n",(0,i.jsx)(t.p,{children:"For query-time this is obviously the most crucial step to perform. Here we query the (hybrid) database to fetch documents (chunks) that are relevant to the user's question. This is done by firing a SQL query to the Postgres system holding the dense and sparse vectors. The SQL query itself already scores the retrieved document chunks on similarity (using cosime similarity and BM25 score in a 50/50 weighing)."}),"\n",(0,i.jsx)(t.h3,{id:"reranking",children:"Reranking"}),"\n",(0,i.jsxs)(t.p,{children:["One of the issues with a normal RAG pipeline's scoring is that the document chunks are embedded in isolation and so is the user query. While this is relatively fast and can be done asynchronously, an alternative would be to use cross-encoders or other models that embed the document chunk together with the query to capture attention across both. This is what a reranker aims to do. The problem however is that this is too time-consuming to do for the entire document set. As a solution however, we can apply a reranker only on those documents that were retrieved by the regular retrieval process and rerank them. A good practice then is to retrieve a relatively large set directly from the database and then return a smaller subset of documents after reranking. RAG Me Up uses ",(0,i.jsx)(t.a,{href:"https://github.com/PrithivirajDamodaran/FlashRank",children:"flashrank"})," for reranking as an optional step."]}),"\n",(0,i.jsx)(t.h3,{id:"answer-check---llm-as-a-judge",children:"Answer Check - LLM-as-a-judge"}),"\n",(0,i.jsxs)(t.p,{children:["Once documents are retrieved and ranked, we can use a form of self-inflection or LLM-as-a-judge to determine whether or not the currently retrieved set of chunks ",(0,i.jsx)(t.em,{children:"can"})," accurately answer the user's question. If this is not the case, we can rewrite the original user's question in an attempt to obtain better document chunks in a new retrieval round. RAG Me Up allows this rewriting loop to be turned on as an optional step and will only perform it once to prevent too lengthy or even infinite rewrites."]}),"\n",(0,i.jsx)(t.h3,{id:"re2",children:"Re2"}),"\n",(0,i.jsxs)(t.p,{children:["It has been shown that ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2309.06275",children:"instructing an LLM to re-read a question"})," - called Re2 - benefits the quality of the answer given by the LLM. Re2 is an optional step in RAG Me Up."]}),"\n",(0,i.jsx)(t.h3,{id:"prompt-creation",children:"Prompt Creation"}),"\n",(0,i.jsx)(t.p,{children:"When the question is potentially remodeled and the documents are finalized to be inject, RAG Me Up will set up a prompt with documents inserted into it and feed the full prompt to the LLM and get the reply back."}),"\n",(0,i.jsx)(t.h3,{id:"provenance-attribution",children:"Provenance Attribution"}),"\n",(0,i.jsxs)(t.p,{children:["Once the LLM has given an answer to the user's question, a RAG pipeline usually finalizes its query-phase and returns it to the client. In some applications of RAG however, it can be crucial to understand what documents were ",(0,i.jsx)(t.strong,{children:"actually"}),' used by the LLM to generate the answer. While, even after reranking, we feed a set of "as-relevant-as-possible" documents to the LLM it may still choose to use some more than others or even ignore some altogether.']}),"\n",(0,i.jsx)(t.p,{children:"Provenance attribtuion tries to assign a score to each document chunk to indicate how relevant it was in generating the answer in hindsight. There are different ways of doing this and RAG Me Up provides provenance attribution as an optional step."}),"\n","\n",(0,i.jsxs)(t.section,{"data-footnotes":!0,className:"footnotes",children:[(0,i.jsx)(t.h2,{className:"sr-only",id:"footnote-label",children:"Footnotes"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{id:"user-content-fn-1",children:["\n",(0,i.jsxs)(t.p,{children:["While overly simplified; LMs are Language Models like the BERT-family of models and are good are converting text into semantic-preserving vectors. They are not to be confused with LLMs (Large Language Models) which generally ",(0,i.jsx)(t.em,{children:"generate"})," text. This goed without saying that LLMs ",(0,i.jsx)(t.em,{children:"also"})," create vectors for any text and hence can often be used as embedding models too. ",(0,i.jsx)(t.a,{href:"#user-content-fnref-1","data-footnote-backref":"","aria-label":"Back to reference 1",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n"]}),"\n"]})]})}function l(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8979:(e,t,n)=>{n.d(t,{A:()=>s});const s=n.p+"assets/images/ragmeup.drawio-f39f8b7c53e6c45e5cf02c9af7acb598.svg"},8453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>a});var s=n(6540);const i={},r=s.createContext(i);function o(e){const t=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:t},e.children)}}}]);