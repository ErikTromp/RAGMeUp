"use strict";(self.webpackChunkrag_me_up_documentation=self.webpackChunkrag_me_up_documentation||[]).push([[263],{1877:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"intro_rag","title":"An introduction to RAG (and AI)","description":"The field of AI obviously stretches across many tasks but the one we will be focusing on right now is that of Natural Language Processing (NLP).","source":"@site/docs/intro_rag.md","sourceDirName":".","slug":"/intro_rag","permalink":"/intro_rag","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/intro_rag.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"RAG Me Up Documentation","permalink":"/"},"next":{"title":"High-Level Design","permalink":"/architecture"}}');var s=t(4848),r=t(8453);const a={sidebar_position:2},i="An introduction to RAG (and AI)",l={},d=[];function c(e){const n={a:"a",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",section:"section",strong:"strong",sup:"sup",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"an-introduction-to-rag-and-ai",children:"An introduction to RAG (and AI)"})}),"\n",(0,s.jsx)(n.p,{children:"The field of AI obviously stretches across many tasks but the one we will be focusing on right now is that of Natural Language Processing (NLP).\r\nIn particular, given that we are looking into AI and RAG, we will focus on the use of Large Language Models (LLMs). LLMs typically come in different\r\nforms and shapes and serve different purposes. While an over-simplification, for the remainder of the documentation we will assume that LLMs can be\r\none of the following:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["A foundation model - This is a model that is trained for many hours across a lot of GPUs to learn a probabilistic model on language. The result\r\nof this task is a model that is able to generate a new token",(0,s.jsx)(n.sup,{children:(0,s.jsx)(n.a,{href:"#user-content-fn-1",id:"user-content-fnref-1","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"1"})})," given a history of already produced tokens. It does so by looking at the history\r\nof produced tokens and deciding what is the most ",(0,s.jsx)(n.strong,{children:"probable"})," next token to produce. This next token is then fed back into the LLM, becoming part\r\nof the new history of the next step, to produce the next token. This behavior is called ",(0,s.jsx)(n.em,{children:"auto-regression"}),". Foundation models are capable of producing\r\ntext that we as humans can read but they generally lack an interactive nature that is required for RAG. Examples of foundation models are:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E",children:"LLaMa 4"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://huggingface.co/google/gemma-3-27b-it",children:"Gemma 3"})}),"\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://openai.com/index/introducing-gpt-4-5/",children:"GPT 4.5"})}),"\n",(0,s.jsx)(n.li,{children:"And many more like Claude, Grok, Gemini, etc."}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["An instruct model - While foundation models are trained to just generate the next token given previous ones, for RAG and chatting in general, we\r\nneed a model that doesn't just complete what we are saying, we need it to ",(0,s.jsx)(n.em,{children:"respond"})," or ",(0,s.jsx)(n.em,{children:"follow our instruction"}),". To this end, models are trained that\r\nnot just predict the next token of any given text corpus, but predict the next token of an answer or reply when given a question. It is a training step\r\non top of foundation models to make them suitable for chatting. ",(0,s.jsx)(n.strong,{children:"For RAG, these are the type of models we are mostly interested in."})]}),"\n",(0,s.jsxs)(n.li,{children:["Alignment-tuned models (RLHF, DPO, CPO, etc.) - These are ",(0,s.jsx)(n.em,{children:"instruct"})," models that are tuned towards a specific domain or task. Originally, by sampling\r\na set of candidate answers with each user question allows to decide which of those answers is preferred over the other answers. This way, the instruct\r\nmodel not only learns to answer questions but also knows what answers better suit certain questions. Since alignment-tuned models are just instruct\r\nmodels further aligned, they can equally well be used for RAG."]}),"\n",(0,s.jsxs)(n.li,{children:["Reasoning models - When we move beyond just simple instruction following models and use reasoning models. While their inner workings are far more\r\ncomplex, a simple way of looking at them is to see them as models that have a number of ",(0,s.jsx)(n.em,{children:"internal instruct"}),' steps where they first ask (themselves)\r\nto come up with a gameplan of answering the user query and then execute that gameplan, evaluating as they go along until they are "confident" enough\r\nthey can answer the original user question. While reasoning models can be considered the "strongest" type of model to date, they are not generally well-suited\r\nfor RAG as they degrade user experience because of the lengthy reasoning process.']}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["For the remainder of talking about RAG Me Up, we will be using instruct models, either from commercial providers or from Huggingface models",(0,s.jsx)(n.sup,{children:(0,s.jsx)(n.a,{href:"#user-content-fn-2",id:"user-content-fnref-2","data-footnote-ref":!0,"aria-describedby":"footnote-label",children:"2"})}),"."]}),"\n","\n",(0,s.jsxs)(n.section,{"data-footnotes":!0,className:"footnotes",children:[(0,s.jsx)(n.h2,{className:"sr-only",id:"footnote-label",children:"Footnotes"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{id:"user-content-fn-1",children:["\n",(0,s.jsxs)(n.p,{children:["A ",(0,s.jsx)(n.em,{children:"token"})," is a term often used in the LLM world and represents a combination of (potentially system or non-readable) characters. It is not the same as a word. ",(0,s.jsx)(n.a,{href:"#user-content-fnref-1","data-footnote-backref":"","aria-label":"Back to reference 1",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{id:"user-content-fn-2",children:["\n",(0,s.jsxs)(n.p,{children:["When using models from Huggingface, make sure you choose an instruct model. ",(0,s.jsx)(n.a,{href:"#user-content-fnref-2","data-footnote-backref":"","aria-label":"Back to reference 2",className:"data-footnote-backref",children:"\u21a9"})]}),"\n"]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>i});var o=t(6540);const s={},r=o.createContext(s);function a(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);