logging_level=DEBUG

embedding_model=avsolatorio/GIST-small-Embedding-v0
embedding_cpu=False

data_directory='data'
file_types="pdf,json,docx,pptx,xslx,csv,xml,txt"
json_schema="."
csv_seperator=";"

splitter='RecursiveCharacterTextSplitter'
recursive_splitter_chunk_size=2048
recursive_splitter_chunk_overlap=128
semantic_chunker_breakpoint_threshold_type=percentile
semantic_chunker_breakpoint_threshold_amount=None
semantic_chunker_number_of_chunks=None
paragraph_chunker_max_chunk_size=512
paragraph_chunker_paragraph_separator="\n\s*\n"

postgres_uri="postgresql://langchain:langchain@localhost:6024/langchain"
vector_store_k=10

use_rrf=False
rrf_k=60

rerank=True
rerank_k=3
rerank_model=ms-marco-MiniLM-L-12-v2

use_hyde=True
hyde_query="You are an expert in generating hypothetical documents that would answer a given question. Generate a document of at most 1000 characters that would contain the answer to the following user question:\n\n{question}"

use_summarization=False
summarization_threshold=100000
summarization_query="Summarize the following conversation. Be succinct but don't leave out any crucial details. Always stick to the same language as was used in the conversation.\n\n{history}"
summarization_encoder="gpt-4o"

temperature=0.2
rag_instruction="Instruction: You are a digital librarian that can answer generic questions on relevant content quickly and succinctly. Here are a few documents from the library that you can use to answer the user's question, retrieved as documents from a database. Be sure to motivate your answer and always mention your source, so which of the documents you used to formulate the answer:

{context}"
rag_question_initial="The initial question you have to answer:

{question}"
rag_question_followup="The follow-up question you have to answer:

{question}"
rag_fetch_new_question="Instruction: You are a digital librarian with a database that contains relevant documents for user queries. Users want to ask questions based on those documents and ask questions that either need you to fetch new documents from the database or that are a followup question on previously obtained documents. You need to decide whether you are going to fetch new documents or whether the user is asking a follow-up question.\nShould new documents be fetched from the database based on this user query? Answer with yes (new documents should be fetched) or no (the current documents can be used to answer the user's latest question) followed by a motivation. The user question is the following: \"{question}\"\n"

use_rewrite_loop=True
rewrite_query_instruction="You have to answer a user question based on documents retrieved from a document database. It is your task to decide whether or not the documents contain the answer to the user's query. You can always only answer with exactly yes or no. The documents that are currently fetched from the database are:

{context}"
rewrite_query_question="The user's question is:

{question}"
rewrite_query_prompt="You are given a user query that should be answered by looking up documents that from a document store using a distance based similarity measure. The documents fetched from the document store were found to be irrelevant to answer the question. Rewrite the following question into an alternative that increases the likelihood of finding relevant documents from the database. You may only answer with the exact rephrasing. The original question is: {question}\n\nThe motivation for not finding the answer to the given question is: {motivation}"

use_re2=True
re2_prompt="Read the question again: "

provenance_method=rerank
provenance_similarity_llm=answerdotai/ModernBERT-base
provenance_include_query=False
provenance_llm_prompt="Instruction: You are a provenance auditor that needs to exactly determine how much an answer given to a user question was based on a given input document, knowing that more than just that one document were considered. Documents may be fully used verbatim, partially used or even translated. You need to give a score indicating how much a source document was used in creating the answer given to a user query, this score must be 0 = source document is not used at all, 1 = barely used, 2 = moderately used, 3 = mostly used, 4 = almost fully used and 5 = full text included in answer. You are forced to always answer only with the score from 0 to 5, don't explain yourself or add more text than just the score.
{query}
The answer given is to this user query is:

{answer}

The source document that you need to score is the following:

{context}"


use_openai=False
openai_model_name='gpt-4.1'

use_azure=False

use_gemini=False
gemini_model_name='gemini-pro'

use_anthropic=False
anthropic_model_name='claude-3-7-sonnet-20250219'

use_ollama=True
ollama_model_name='gemma3:12b'